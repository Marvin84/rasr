<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Sprint: GradientCheck.cc Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Sprint
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
      <li><a href="globals.html"><span>File&#160;Members</span></a></li>
    </ul>
  </div>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_f47822d91ab238130283b876d44a382d.html">Nn</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">GradientCheck.cc</div>  </div>
</div><!--header-->
<div class="contents">
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="comment">/** Copyright 2020 RWTH Aachen University. All rights reserved.</span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="comment"> *</span></div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;<span class="comment"> *  Licensed under the RWTH ASR License (the &quot;License&quot;);</span></div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment"> *  you may not use this file except in compliance with the License.</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="comment"> *  You may obtain a copy of the License at</span></div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment"> *</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="comment"> *      http://www.hltpr.rwth-aachen.de/rwth-asr/rwth-asr-license.html</span></div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="comment"> *</span></div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment"> *  Unless required by applicable law or agreed to in writing, software</span></div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="comment"> *  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment"> *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="comment"> *  See the License for the specific language governing permissions and</span></div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="comment"> *  limitations under the License.</span></div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="comment"> */</span></div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="preprocessor">#include &quot;FeedForwardTrainer.hh&quot;</span></div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="keyword">namespace </span><a class="code" href="namespaceNn.html">Nn</a> {</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;T FeedForwardTrainer&lt;T&gt;::getNewError() {</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;    <span class="comment">// Get error.</span></div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;    criterion().reinputWithNewNnOutput(network().getTopLayerOutput());</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;    T newError = 0;</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;    criterion().getObjectiveFunction(newError);</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;    <span class="comment">// apply regularization only when not in batch mode</span></div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;    <span class="keywordflow">if</span> (!estimator().fullBatchMode()) {</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;        u32 batchSize = network().getLayerInput(0)[0]-&gt;nColumns();</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;        newError += regularizer().objectiveFunction(network(), T(batchSize));</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;    }</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;    <span class="keywordflow">return</span> newError;</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;}</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> Params&gt;</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;<span class="keywordtype">void</span> FeedForwardTrainer&lt;T&gt;::gradientCheckComponent(</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;        T grad, T* paramPtr, Params&amp; params, u32 layerIdx) {</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;    u32 paramIdx = paramPtr - params.begin();</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;    <span class="comment">// First calculate the symmetric numeric gradient ((f(x + h) - f(x - h)) / (2 h)).</span></div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;    <span class="keyword">const</span> <span class="keywordtype">int</span> precision    = gradientCheckPrecision_;</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;    <span class="keyword">const</span> T   perturbation = gradientCheckPerturbation_;</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;    <span class="keyword">const</span> T   diffs[]      = {perturbation, -perturbation};</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;    T         origParam    = 0;</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;    T         errors[]     = {0, 0};</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;</div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;    <span class="keywordflow">for</span> (<span class="keywordtype">short</span> i = 0; i &lt;= 2; ++i) {</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;        <span class="comment">// We always sync the whole vector/matrix. This is of course very inefficient,</span></div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;        <span class="comment">// but it would require some more complicated syncing code otherwise,</span></div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;        <span class="comment">// and is probably anyway not the bottleneck when doing the gradient check.</span></div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;        <span class="comment">// We expect to be not in computation mode here.</span></div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;        <span class="keywordflow">if</span> (i == 0)</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;            origParam = *paramPtr;</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;        <span class="keywordflow">if</span> (i == 2)</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;            *paramPtr = origParam;</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;        <span class="keywordflow">else</span></div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;            *paramPtr = origParam + diffs[i];</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;        params.initComputation(<span class="keyword">true</span>);  <span class="comment">// sync to GPU</span></div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;        <span class="keywordflow">if</span> (i &lt; 2) {</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;            <span class="comment">// Use already set features.</span></div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;            network().forwardLayers(layerIdx);</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;            <span class="comment">// Get error.</span></div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;            errors[i] = getNewError();</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;            <a class="code" href="Assertions_8hh.html#ae925d7587e34145cd52f674b2012539a">require</a>(!criterion().discardCurrentInput());</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;        }</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;        params.finishComputation(<span class="keyword">false</span>);  <span class="comment">// we expect the CPU memory to be correct</span></div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;    }</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;    T numericGrad = (errors[0] - errors[1]) / (2 * perturbation);</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;    <span class="comment">// Now check with the gradient which we got before from the criterion.</span></div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;    <span class="comment">// Threshold based on minimum grad and precision.</span></div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;    T threshold = (T)pow(T(10.0), std::max&lt;T&gt;(T(0.0), <a class="code" href="namespaceCore.html#acc0985bf68d2a0d96bec943661eb48a6">ceil</a>(log10(std::min(fabs(grad), fabs(numericGrad))))) - precision);</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;    T diff      = fabs(grad - numericGrad);</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;    ((<a class="code" href="namespaceMath.html#a7f4a7868ba587405f51a7f3ff5a8efc7">Math::isnan</a>(diff) || diff &gt; threshold) ? <a class="code" href="classCore_1_1Component.html#ac0806de1942b78cd299ea032c895db7a">Core::Component::warning</a>(<span class="stringliteral">&quot;Gradient check failed: &quot;</span>) : <a class="code" href="classCore_1_1Component.html#a6f28c4d1852f247c233a8955bd91c19b">Core::Component::log</a>(<span class="stringliteral">&quot;Gradient check succeeded: &quot;</span>))</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;            &lt;&lt; <span class="stringliteral">&quot;paramIdx: &quot;</span> &lt;&lt; paramIdx</div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;            &lt;&lt; <span class="stringliteral">&quot;, param: &quot;</span> &lt;&lt; origParam</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;            &lt;&lt; <span class="stringliteral">&quot;, grad: &quot;</span> &lt;&lt; grad</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;            &lt;&lt; <span class="stringliteral">&quot;, numericGrad: &quot;</span> &lt;&lt; numericGrad</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;            &lt;&lt; <span class="stringliteral">&quot; (leftError: &quot;</span> &lt;&lt; errors[0]</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;            &lt;&lt; <span class="stringliteral">&quot;, rightError: &quot;</span> &lt;&lt; errors[1] &lt;&lt; <span class="stringliteral">&quot;)&quot;</span>;</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;}</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;<span class="keywordtype">void</span> FeedForwardTrainer&lt;T&gt;::gradientCheck() {</div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;    <span class="comment">// Only check gradient components / parameters of last layer.</span></div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;    <span class="comment">// The gradient check is mostly to check the derivation of the criterion.</span></div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;    s32                    layerIdx = (s32)network().nLayers() - 1;</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;    <a class="code" href="classNeuralNetworkLayer.html">NeuralNetworkLayer&lt;T&gt;</a>&amp; layer    = network().getLayer(layerIdx);</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;    <a class="code" href="Assertions_8hh.html#ae925d7587e34145cd52f674b2012539a">require</a>(layer.isTrainable());</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;    <span class="keywordflow">if</span> (<span class="keyword">typename</span> Types&lt;T&gt;::NnVector* bias = layer.getBias()) {</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;        bias-&gt;finishComputation(<span class="keyword">true</span>);  <span class="comment">// sync to CPU</span></div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;        <span class="keyword">typename</span> Types&lt;T&gt;::NnVector&amp; gradientBias = statistics().gradientBias(layerIdx);</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;        gradientBias.finishComputation(<span class="keyword">true</span>);  <span class="comment">// sync to CPU</span></div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;        <span class="keywordflow">for</span> (T* gradPtr = gradientBias.begin(); gradPtr != gradientBias.end(); ++gradPtr) {</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;            T* paramPtr = bias-&gt;begin() + (gradPtr - gradientBias.begin());</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;            gradientCheckComponent(*gradPtr, paramPtr, *bias, layerIdx);</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;        }</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;        gradientBias.initComputation(<span class="keyword">false</span>);</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;        bias-&gt;initComputation(<span class="keyword">false</span>);</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;    }</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;</div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;    <span class="keywordflow">for</span> (u32 stream = 0; stream &lt; statistics().gradientWeights(layerIdx).size(); stream++) {</div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;        <span class="keyword">typename</span> Types&lt;T&gt;::NnMatrix* weights = layer.getWeights(stream);</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;        <span class="keywordflow">if</span> (!weights)</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;            <span class="keywordflow">continue</span>;</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;        weights-&gt;finishComputation(<span class="keyword">true</span>);  <span class="comment">// sync to GPU</span></div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;        <span class="keyword">typename</span> Types&lt;T&gt;::NnMatrix&amp; gradientWeights = statistics().gradientWeights(layerIdx)[stream];</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;        gradientWeights.finishComputation(<span class="keyword">true</span>);  <span class="comment">// sync to GPU</span></div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;</div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;        <span class="keywordflow">for</span> (T* gradPtr = gradientWeights.begin(); gradientWeights.end(); ++gradPtr) {</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;            T* paramPtr = weights-&gt;begin() + (gradPtr - gradientWeights.begin());</div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;</div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;            gradientCheckComponent(*gradPtr, paramPtr, *weights, layerIdx);</div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;        }</div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;        gradientWeights.initComputation(<span class="keyword">false</span>);</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;        weights-&gt;initComputation(<span class="keyword">false</span>);</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;    }</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;}</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;<span class="comment">// Calculates grad^T * grad * learningRate.</span></div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;T FeedForwardTrainer&lt;T&gt;::getDirectionalEstimate() {</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;    T learningRate           = estimator().learningRate();</div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;    T biasLearningRateFactor = estimator().biasLearningRateFactor();</div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;</div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;    T sum = 0;</div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;    <span class="keywordflow">for</span> (s32 layerIdx = 0; layerIdx &lt; (s32)network().nLayers(); ++layerIdx) {</div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;        <a class="code" href="classNeuralNetworkLayer.html">NeuralNetworkLayer&lt;T&gt;</a>&amp; layer = network().getLayer(layerIdx);</div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;        <span class="keywordflow">if</span> (!layer.isTrainable())</div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;            <span class="keywordflow">continue</span>;</div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;</div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;        T layerLearningRateFactor = layer.learningRate();</div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;</div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;        <span class="keywordflow">if</span> (layer.getBias()) {</div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;            T                            localLearningRate = layerLearningRateFactor * biasLearningRateFactor;</div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;            <span class="keyword">typename</span> Types&lt;T&gt;::NnVector&amp; gradientBias      = statistics().gradientBias(layerIdx);</div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;            sum += gradientBias.sumOfSquares() * localLearningRate;</div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;        }</div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;</div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;        <span class="keywordflow">for</span> (u32 stream = 0; stream &lt; statistics().gradientWeights(layerIdx).size(); stream++) {</div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;            <span class="keywordflow">if</span> (!layer.getWeights(stream))</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;                <span class="keywordflow">continue</span>;</div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;            T                            localLearningRate = layerLearningRateFactor;</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;            <span class="keyword">typename</span> Types&lt;T&gt;::NnMatrix&amp; gradientWeights   = statistics().gradientWeights(layerIdx)[stream];</div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;            sum += gradientWeights.sumOfSquares() * localLearningRate;</div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;        }</div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;    }</div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;</div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;    <span class="keywordflow">return</span> sum * learningRate;</div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;}</div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;</div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;<span class="keywordtype">void</span> FeedForwardTrainer&lt;T&gt;::simpleGradientCheck(T oldError) {</div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;    <span class="keyword">const</span> <span class="keywordtype">int</span> precision = gradientCheckPrecision_;</div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;    <span class="comment">// We expect the Steepest-Descent-Estimator to know the estimator step.</span></div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;    <span class="keyword">auto</span>* est = <span class="keyword">dynamic_cast&lt;</span>SteepestDescentEstimator&lt;T&gt;*<span class="keyword">&gt;</span>(&amp;estimator());</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;    <span class="keywordflow">if</span> (!est) {</div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;        <a class="code" href="classCore_1_1Component.html#a61fed12dd2c06c8713eaf10264735912">Core::Component::error</a>(<span class="stringliteral">&quot;simple gradient check: need steepest-descent-estimator&quot;</span>);</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;        <span class="keywordflow">return</span>;</div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;    }</div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;    <span class="keywordflow">if</span> (!est-&gt;isDefaultConfig()) {</div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;        <a class="code" href="classCore_1_1Component.html#a61fed12dd2c06c8713eaf10264735912">Core::Component::error</a>(<span class="stringliteral">&quot;simple gradient check: need steepest-descent-estimator with default config, &quot;</span>)</div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;                &lt;&lt; <span class="stringliteral">&quot;i.e. no decay, no momentum&quot;</span>;</div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;        <span class="keywordflow">return</span>;</div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;    }</div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;</div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;    <span class="comment">// Get error.</span></div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;    T newError = getNewError();</div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;    <a class="code" href="Assertions_8hh.html#ae925d7587e34145cd52f674b2012539a">require</a>(!criterion().discardCurrentInput());</div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;</div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;    <span class="comment">// Now calculate grad^T * grad * learningRate, which is an estimation of oldError - newError.</span></div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;    T numericStep = getDirectionalEstimate();</div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;    T realStep    = oldError - newError;</div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;    <span class="comment">// Threshold based on minimum grad and precision.</span></div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;    T threshold = (T)pow(T(10.0), std::max&lt;T&gt;(T(0.0), <a class="code" href="namespaceCore.html#acc0985bf68d2a0d96bec943661eb48a6">ceil</a>(log10(std::min(fabs(realStep), fabs(numericStep))))) - precision);</div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;    T diff      = fabs(realStep - numericStep);</div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;    <span class="keywordflow">if</span> (<a class="code" href="namespaceMath.html#a7f4a7868ba587405f51a7f3ff5a8efc7">Math::isnan</a>(diff) || diff &gt; threshold) {</div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;        <a class="code" href="classCore_1_1Component.html#ac0806de1942b78cd299ea032c895db7a">Core::Component::warning</a>(<span class="stringliteral">&quot;Simple gradient check failed: &quot;</span>)</div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;                &lt;&lt; <span class="stringliteral">&quot;oldError: &quot;</span> &lt;&lt; oldError</div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;                &lt;&lt; <span class="stringliteral">&quot;, newError: &quot;</span> &lt;&lt; newError</div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;                &lt;&lt; <span class="stringliteral">&quot;, errStep: &quot;</span> &lt;&lt; (oldError - newError)</div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;                &lt;&lt; <span class="stringliteral">&quot;, numeric errStep: &quot;</span> &lt;&lt; numericStep;</div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;    }</div><div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;    <span class="keywordflow">else</span></div><div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;        <a class="code" href="classCore_1_1Component.html#a6f28c4d1852f247c233a8955bd91c19b">Core::Component::log</a>(<span class="stringliteral">&quot;Simple gradient check succeeded&quot;</span>);</div><div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;}</div><div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;</div><div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div><div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;<span class="keywordtype">bool</span> FeedForwardTrainer&lt;T&gt;::convergenceCheckRepeat(T&amp; error, NnMatrix&amp; errorSignal) {</div><div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;    <span class="comment">// Get new error.</span></div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;    T newError = getNewError();</div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;    <a class="code" href="Assertions_8hh.html#ae925d7587e34145cd52f674b2012539a">require</a>(!criterion().discardCurrentInput());</div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;    T errDiff      = error - newError;</div><div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;    T learningRate = estimator().learningRate();</div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;    <span class="keywordflow">if</span> (errDiff &lt; 0) {</div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;        <a class="code" href="classCore_1_1Component.html#ac0806de1942b78cd299ea032c895db7a">Core::Component::warning</a>(<span class="stringliteral">&quot;Convergence check: error got worse: &quot;</span>)</div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;                &lt;&lt; <span class="stringliteral">&quot;oldError: &quot;</span> &lt;&lt; error</div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;                &lt;&lt; <span class="stringliteral">&quot;, newError: &quot;</span> &lt;&lt; newError;</div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;        require_lt(convergenceCheckLearningRateFactor_, 1);</div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;        require_gt(convergenceCheckLearningRateFactor_, 0);</div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;        <a class="code" href="classCore_1_1Component.html#a6f28c4d1852f247c233a8955bd91c19b">Core::Component::log</a>(<span class="stringliteral">&quot;lowering learning rate, &quot;</span>)</div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;                &lt;&lt; <span class="stringliteral">&quot;current: &quot;</span> &lt;&lt; learningRate;</div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;        learningRate *= convergenceCheckLearningRateFactor_;</div><div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;        <a class="code" href="classCore_1_1Component.html#a6f28c4d1852f247c233a8955bd91c19b">Core::Component::log</a>(<span class="stringliteral">&quot;new learning rate: &quot;</span>) &lt;&lt; learningRate;</div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;        require_gt(learningRate, <a class="code" href="structCore_1_1Type.html">Core::Type&lt;T&gt;::delta</a> * 2);</div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;        estimator().setLearningRate(learningRate);</div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;    }</div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;    <span class="keywordflow">else</span> {  <span class="comment">// errDiff &gt;= 0</span></div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;        <span class="keywordflow">if</span> (errDiff &gt; 0)</div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;            <a class="code" href="classCore_1_1Component.html#a6f28c4d1852f247c233a8955bd91c19b">Core::Component::log</a>(<span class="stringliteral">&quot;Convergence check: new lower error: &quot;</span>)</div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;                    &lt;&lt; newError</div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;                    &lt;&lt; <span class="stringliteral">&quot; (oldError: &quot;</span> &lt;&lt; error &lt;&lt; <span class="stringliteral">&quot;, diff: &quot;</span> &lt;&lt; errDiff &lt;&lt; <span class="stringliteral">&quot;)&quot;</span>;</div><div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;        <span class="keywordflow">else</span></div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;            <a class="code" href="classCore_1_1Component.html#a6f28c4d1852f247c233a8955bd91c19b">Core::Component::log</a>(<span class="stringliteral">&quot;Convergence check: no error diff&quot;</span>)</div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;                    &lt;&lt; <span class="stringliteral">&quot; error: &quot;</span> &lt;&lt; newError;</div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;</div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;        <span class="comment">// Calculate the 2-norm^2 of the error signal.</span></div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;        <span class="comment">// getDirectionalEstimate() mostly does this, except the learning rate factor.</span></div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;        <span class="comment">// Note that we cannot check whether the error (objective function) is zero,</span></div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;        <span class="comment">// because the error function does not necessarily have the minima zero.</span></div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;        T errorNorm = getDirectionalEstimate();</div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;        <span class="keywordflow">if</span> (errorNorm &lt; convergenceCheckGradNormLimit_ || <a class="code" href="namespaceCore.html#a100585c516d2491add7c5826237f4cef">Core::isAlmostEqualUlp</a>(errDiff, 0, 20)) {</div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;            <a class="code" href="classCore_1_1Component.html#a6f28c4d1852f247c233a8955bd91c19b">Core::Component::log</a>(<span class="stringliteral">&quot;Convergence check: stopping with gradient norm: &quot;</span>)</div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;                    &lt;&lt; errorNorm;</div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;            <span class="keywordflow">return</span> <span class="keyword">false</span>;</div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;        }</div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;        <span class="keywordflow">else</span> {</div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;            <a class="code" href="classCore_1_1Component.html#a6f28c4d1852f247c233a8955bd91c19b">Core::Component::log</a>(<span class="stringliteral">&quot;Convergence check: gradient norm: &quot;</span>)</div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;                    &lt;&lt; errorNorm;</div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;            <span class="comment">// Repeat. Fall through.</span></div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;        }</div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;    }</div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;</div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;    <span class="comment">// Get new error signal and error.</span></div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;    <span class="comment">// In getNewError(), we refeeded the current NN output to the criterion.</span></div><div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;    criterion().getErrorSignal_naturalPairing(errorSignal, network().getTopLayer());</div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;    error = 0;</div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;    criterion().getObjectiveFunction(error);</div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;</div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;    <span class="comment">// Reset statistics (error + gradients).</span></div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;    statistics().addToObjectiveFunction(-statistics().objectiveFunction());</div><div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;    <span class="keywordflow">for</span> (s32 layerIdx = lowestTrainableLayerIndex_; layerIdx &lt; (s32)network().nLayers(); ++layerIdx) {</div><div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;        <span class="keywordflow">if</span> (!network().getLayer(layerIdx).isTrainable())</div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;            <span class="keywordflow">continue</span>;</div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;        statistics().gradientBias(layerIdx).setToZero();</div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;        <span class="keywordflow">for</span> (u32 stream = 0; stream &lt; statistics().gradientWeights(layerIdx).size(); stream++) {</div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;            statistics().gradientWeights(layerIdx)[stream].setToZero();</div><div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;        }</div><div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;    }</div><div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;</div><div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;    <span class="comment">// Repeat.</span></div><div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;    <span class="keywordflow">return</span> <span class="keyword">true</span>;</div><div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;}</div><div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;</div><div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;<span class="comment">// explicit template instantiation</span></div><div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;<span class="keyword">template</span> <span class="keyword">class </span>FeedForwardTrainer&lt;f32&gt;;</div><div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;<span class="keyword">template</span> <span class="keyword">class </span>FeedForwardTrainer&lt;f64&gt;;</div><div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;</div><div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;}  <span class="comment">// namespace Nn</span></div><div class="ttc" id="namespaceCore_html_acc0985bf68d2a0d96bec943661eb48a6"><div class="ttname"><a href="namespaceCore.html#acc0985bf68d2a0d96bec943661eb48a6">Core::ceil</a></div><div class="ttdeci">float ceil(float v)</div><div class="ttdoc">Core::ceil : wrapper for several ceil functions. </div><div class="ttdef"><b>Definition:</b> <a href="Core_2Utility_8hh_source.html#l00206">Core/Utility.hh:206</a></div></div>
<div class="ttc" id="classCore_1_1Component_html_a61fed12dd2c06c8713eaf10264735912"><div class="ttname"><a href="classCore_1_1Component.html#a61fed12dd2c06c8713eaf10264735912">Core::Component::error</a></div><div class="ttdeci">Message error(const char *msg=0,...) const __attribute__((format(printf</div><div class="ttdoc">Print an error message. </div><div class="ttdef"><b>Definition:</b> <a href="Core_2Component_8cc_source.html#l00240">Core/Component.cc:240</a></div></div>
<div class="ttc" id="namespaceMath_html_a7f4a7868ba587405f51a7f3ff5a8efc7"><div class="ttname"><a href="namespaceMath.html#a7f4a7868ba587405f51a7f3ff5a8efc7">Math::isnan</a></div><div class="ttdeci">bool isnan(T val)</div><div class="ttdoc">isnan(val) returns true if val is nan. </div></div>
<div class="ttc" id="namespaceNn_html"><div class="ttname"><a href="namespaceNn.html">Nn</a></div><div class="ttdoc">Copyright 2020 RWTH Aachen University. </div><div class="ttdef"><b>Definition:</b> <a href="ActivationLayer_8cc_source.html#l00523">ActivationLayer.cc:523</a></div></div>
<div class="ttc" id="classCore_1_1Component_html_ac0806de1942b78cd299ea032c895db7a"><div class="ttname"><a href="classCore_1_1Component.html#ac0806de1942b78cd299ea032c895db7a">Core::Component::warning</a></div><div class="ttdeci">Message warning(const char *msg=0,...) const __attribute__((format(printf</div><div class="ttdoc">Print a warning message. </div><div class="ttdef"><b>Definition:</b> <a href="Core_2Component_8cc_source.html#l00225">Core/Component.cc:225</a></div></div>
<div class="ttc" id="classCore_1_1Component_html_a6f28c4d1852f247c233a8955bd91c19b"><div class="ttname"><a href="classCore_1_1Component.html#a6f28c4d1852f247c233a8955bd91c19b">Core::Component::log</a></div><div class="ttdeci">Message log(const char *msg,...) const __attribute__((format(printf</div><div class="ttdoc">Print an information message. </div><div class="ttdef"><b>Definition:</b> <a href="Core_2Component_8cc_source.html#l00204">Core/Component.cc:204</a></div></div>
<div class="ttc" id="Assertions_8hh_html_ae925d7587e34145cd52f674b2012539a"><div class="ttname"><a href="Assertions_8hh.html#ae925d7587e34145cd52f674b2012539a">require</a></div><div class="ttdeci">#define require(expr)</div><div class="ttdoc">Check precondition. </div><div class="ttdef"><b>Definition:</b> <a href="WeightedAccumulator_8hh_source.html#l00210">WeightedAccumulator.hh:210</a></div></div>
<div class="ttc" id="structCore_1_1Type_html"><div class="ttname"><a href="structCore_1_1Type.html">Core::Type</a></div><div class="ttdoc">Static information about elementary types. </div><div class="ttdef"><b>Definition:</b> <a href="Core_2Types_8hh_source.html#l00040">Core/Types.hh:40</a></div></div>
<div class="ttc" id="namespaceCore_html_a100585c516d2491add7c5826237f4cef"><div class="ttname"><a href="namespaceCore.html#a100585c516d2491add7c5826237f4cef">Core::isAlmostEqualUlp</a></div><div class="ttdeci">bool isAlmostEqualUlp(f32 a, f32 b, s32 tolerance)</div><div class="ttdoc">Test for near-equality of floating point numbers with tolerance given in unit of least precision...</div><div class="ttdef"><b>Definition:</b> <a href="Core_2Utility_8hh_source.html#l00384">Core/Utility.hh:384</a></div></div>
<div class="ttc" id="classNeuralNetworkLayer_html"><div class="ttname"><a href="classNeuralNetworkLayer.html">NeuralNetworkLayer</a></div><div class="ttdoc">Copyright 2020 RWTH Aachen University. </div><div class="ttdef"><b>Definition:</b> <a href="Nn__NeuralNetworkLayer_8cc_source.html#l00019">Nn_NeuralNetworkLayer.cc:19</a></div></div>
</div><!-- fragment --></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
