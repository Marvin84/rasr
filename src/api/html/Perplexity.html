<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Sprint: On computing perplexity / cross entropy</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Sprint
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">On computing perplexity / cross entropy </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Definitions: cross-entropy in bit H_p(T) = - {1}{W}  p(T) perplexity PP_p(T) = ( - {1}{W}  p(T) )</p>
<p>where $T$ is the text and $W$ is the length of the text measured in words. If the $s_1, , s_K$ are the sentences of the text, then</p>
<p>p(T) = {k=1}^K p(s_k)</p>
<p>A recurring issue with perplexity is whether or not to count the sentence end. The option chosen here, is to treat the sentence end as a genuine event. I.e. the term p(sentence-end | final words) is included in $p(s)$, and $W$ is the number of "ordinary" words plus one for each sentences. (This is consistent with well-known publications such as [Chen &amp; Goodman 1999].)</p>
<p>Calculating the perplexity is less straight forward than you might think, due to the concept of language model phrases. The idea is that multiple words are treated as a single event by the language model. In <a class="el" href="namespaceBliss.html" title="Copyright 2020 RWTH Aachen University. ">Bliss</a> such a thing is represented as an ortographic form containing one (or more) blank character(s). Now let us consider the following example: first the <a class="el" href="namespaceBliss.html" title="Copyright 2020 RWTH Aachen University. ">Bliss</a> lexicon:</p>
<p>&lt;lemma&gt; &lt;orth&gt;going&lt;/orth&gt; &lt;phon&gt;g  I N&lt;/phon&gt; &lt;/lemma&gt; &lt;lemma&gt; &lt;orth&gt;to&lt;/orth&gt; &lt;phon&gt;t u:&lt;/phon&gt; &lt;/lemma&gt; &lt;lemma&gt; &lt;orth&gt;going to&lt;/orth&gt; &lt;phon&gt;g O n &lt;/phon&gt; &lt;synt&gt;&lt;tok&gt;going_to&lt;/tok&gt;&lt;/synt&gt; &lt;eval&gt;&lt;tok&gt;going&lt;/tok&gt;&lt;tok&gt;to&lt;/tok&gt;&lt;/eval&gt; &lt;/lemma&gt; &lt;lemma&gt; &lt;orth&gt;New York&lt;/orth&gt; &lt;phon&gt;n u: j O: k&lt;/phon&gt; &lt;synt&gt;&lt;tok&gt;[CITY]&lt;/tok&gt;&lt;/synt&gt; &lt;/lemma&gt; &lt;lemma&gt; &lt;orth&gt;[breath]&lt;/orth&gt; &lt;synt&gt; &lt;eval&gt; &lt;/lemma&gt;</p>
<p>Now what about the sentence "I'am going to [breath] visit New York". We observe that the phrase "going to" allows for two parsings, namely as one or as two lemmata, while "New York" can only be read as a single token. (Let's assume that "New" with captial N does not occur in the lexicon.) So there are two possible syntactic token sequences:</p>
<p>I'am going to visit [CITY] &lt;/s&gt; I'am going_to visit [CITY] &lt;/s&gt;</p>
<p>The first question when computing perplexity is: What is the log-likelihood LL = ln(p(W))? The solution implemented below is: Use the most likely token sequence, i.e. LL = max_W ln(p(W)).</p>
<p>The second question is what is the number of words $W$? We might use the length of the most likely token sequence, but this would bias the result in favour of long phrases. We could count the number of blanks (plus one), but this would violate an important design principle of <a class="el" href="namespaceBliss.html" title="Copyright 2020 RWTH Aachen University. ">Bliss</a> and moreover you can argue that "New
  York" should be counted as a single word unless "New" and "York" individually also have the status of a word, which is not true for "New". Non-lexical events like '[breath]" pose another complication: Some system setups model them as language model events others don't. A common formal characterisitic property is that they do not count in evaluation. The solution to these issues as implemented below is: Take the most likely lemma sequence, according to the language model, and count the number of evaluation tokens (plus one for the sentence end). In this example the result is six. Addtionally we provide numbers on the basis of syntactic tokens. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
